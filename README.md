<h1 align="center"> SARATR-X: Towards Building A Foundation Model for SAR Target Recognition </h1> 

<h5 align="center"><em> Weijie Li (æç®æ°), Wei Yang (æ¨å¨), Yuenan Hou (ä¾¯è·ƒå—), Li Liu (åˆ˜ä¸½), Yongxiang Liu (åˆ˜æ°¸ç¥¥), and Xiang Li (é»æ¹˜) </em></h5>

<p align="center">
  <a href="#Introduction">Introduction</a> |
  <a href="#Pre-training">Pre-training</a> |
  <a href="#Classification">Classification</a> |
  <a href="#Detection">Detection</a> |
  <a href="#Statement">Statement</a>
</p >

<p align="center">
<a href="https://arxiv.org/abs/2405.09365"><img src="https://img.shields.io/badge/Paper-arxiv-red"></a>  
</p>

## Introduction

This is the official repository for the paper â€œSARATR-X: Towards Building A Foundation Model for SAR Target Recognitionâ€.

è¿™é‡Œæ˜¯è®ºæ–‡ â€œSARATR-X: Towards Building A Foundation Model for SAR Target Recognition (SARATR-Xï¼šè¿ˆå‘SARç›®æ ‡è¯†åˆ«åŸºç¡€æ¨¡å‹) â€çš„ä»£ç åº“ã€‚

**Abstract:** 
Despite the remarkable progress in synthetic aperture radar automatic target recognition (SAR ATR), recent efforts have concentrated on detecting and classifying a specific category, e.g., vehicles, ships, airplanes, or buildings. One of the fundamental limitations of the top-performing SAR ATR methods is that the learning paradigm is supervised, task-specific, limited-category, closed-world learning, which depends on massive amounts of accurately annotated samples that are expensively labeled by expert SAR analysts and have limited generalization capability and scalability. In this work, we make the first attempt towards building a foundation model for SAR ATR, termed SARATR-X. SARATR-X learns generalizable representations via self-supervised learning (SSL) and provides a cornerstone for label-efficient model adaptation to generic SAR target detection and classification tasks. Specifically, SARATR-X is trained on 0.18 M unlabelled SAR target samples, which are curated by combining contemporary benchmarks and constitute the largest publicly available dataset till now. Considering the characteristics of SAR images, a backbone tailored for SAR ATR is carefully designed, and a two-step SSL method endowed with multi-scale gradient features was applied to ensure the feature diversity and model scalability of SARATR-X. The capabilities of SARATR-X are evaluated on classification under few-shot and robustness settings and detection across various categories and scenes, and impressive performance is achieved, often competitive with or even superior to prior fully supervised, semi-supervised, or self-supervised algorithms. 

**æ‘˜è¦ï¼š** 
å°½ç®¡åˆæˆå­”å¾„é›·è¾¾è‡ªåŠ¨ç›®æ ‡è¯†åˆ«ï¼ˆsynthetic aperture radar automatic target recognition, SAR ATRï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†æœ€è¿‘çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨å¯¹ç‰¹å®šç±»åˆ«ï¼ˆå¦‚è½¦è¾†ã€èˆ¹èˆ¶ã€é£æœºæˆ–å»ºç­‘ç‰©ï¼‰çš„æ£€æµ‹å’Œåˆ†ç±»ä¸Šã€‚æ€§èƒ½è‰¯å¥½çš„ SAR ATR æ–¹æ³•çš„ä¸€ä¸ªåŸºæœ¬å±€é™æ˜¯ï¼Œå…¶å­¦ä¹ èŒƒå¼æ˜¯æœ‰ç›‘ç£çš„ã€ç‰¹å®šä»»åŠ¡çš„ã€æœ‰é™ç±»åˆ«çš„ã€å°é—­ä¸–ç•Œçš„å­¦ä¹ ï¼Œè¿™ç§å­¦ä¹ ä¾èµ–äºå¤§é‡å‡†ç¡®æ ‡æ³¨çš„æ ·æœ¬ï¼Œè€Œè¿™äº›æ ·æœ¬æ˜¯ç”± SAR ä¸“å®¶åˆ†æäººå‘˜èŠ±è´¹é«˜æ˜‚æˆæœ¬æ ‡æ³¨çš„ï¼Œå…¶æ³›åŒ–èƒ½åŠ›å’Œå¯æ‰©å±•æ€§æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡å°è¯•ä¸º SAR ATR å»ºç«‹ä¸€ä¸ªåŸºç¡€æ¨¡å‹ï¼Œç§°ä¸º SARATR-Xã€‚SARATR-X é€šè¿‡è‡ªç›‘ç£å­¦ä¹  (self-supervised learning, SSL) å­¦ä¹ å¯æ³›åŒ–çš„è¡¨å¾ï¼Œä¸ºæ ‡ç­¾é«˜æ•ˆæ¨¡å‹é€‚åº”é€šç”¨ SAR ç›®æ ‡æ£€æµ‹å’Œåˆ†ç±»ä»»åŠ¡æä¾›äº†åŸºçŸ³ã€‚å…·ä½“æ¥è¯´ï¼ŒSARATR-X åœ¨ 0.18 M ä¸ªæœªæ ‡è®°çš„åˆæˆå­”å¾„é›·è¾¾ç›®æ ‡æ ·æœ¬ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œè¿™äº›æ ·æœ¬æ˜¯ç»“åˆå½“ä»£æ•°æ®é›†åŸºå‡†ï¼Œæ„æˆäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å…¬å¼€å¯ç”¨é¢„è®­ç»ƒæ•°æ®é›†ã€‚è€ƒè™‘åˆ°åˆæˆå­”å¾„é›·è¾¾å›¾åƒçš„ç‰¹ç‚¹ï¼Œä¸ºåˆæˆå­”å¾„é›·è¾¾ ATR é‡èº«å®šåˆ¶çš„éª¨æ¶ç»è¿‡äº†ç²¾å¿ƒè®¾è®¡ï¼Œå¹¶é‡‡ç”¨äº†å…·æœ‰å¤šå°ºåº¦æ¢¯åº¦ç‰¹å¾çš„ä¸¤æ­¥ SSL æ–¹æ³•ï¼Œä»¥ç¡®ä¿ SARATR-X çš„ç‰¹å¾å¤šæ ·æ€§å’Œæ¨¡å‹å¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬å¯¹ SARATR-X çš„èƒ½åŠ›è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬å°‘é•œå¤´å’Œé²æ£’æ€§è®¾ç½®ä¸‹çš„åˆ†ç±»ä»¥åŠå„ç§ç±»åˆ«å’Œåœºæ™¯çš„æ£€æµ‹ï¼Œå…¶æ€§èƒ½ä»¤äººå°è±¡æ·±åˆ»ï¼Œé€šå¸¸å¯ä¸ä¹‹å‰çš„å…¨ç›‘ç£ã€åŠç›‘ç£æˆ–è‡ªç›‘ç£ç®—æ³•ç›¸åª²ç¾ï¼Œç”šè‡³æ›´èƒœä¸€ç­¹ã€‚

## Pre-training
Our codes are based on [SAR-JEPA](https://github.com/waterdisappear/SAR-JEPA) and [HiVit](https://github.com/zhangxiaosong18/hivit).

### Requirements:
- Python3
- CUDA 11.1
- PyTorch 1.8+ with CUDA support
- timm 0.5.4
- tensorboard

### Step-by-step installation

```bash
conda create -n saratrx python=3.9 -y
conda activate saratrx
cd pre-training

pip install torch==1.8.2 torchvision==0.9.2 torchaudio==0.8.2 --extra-index-url https://download.pytorch.org/whl/lts/1.8/cu111
pip install timm==0.5.4 tensorboard
pip install -r requirements.txt
```

### Start Pre-training with SAR images
```bash
python -m torch.distributed.launch --nproc_per_node 8 --master_port 12345 --use_env main_pretrain.py
    --data_path <imagenet-path> --output_dir <pertraining-output-path>
    --model mae_hivit_base_dec512d6b --mask_ratio 0.75
    --batch_size 100 --accum_iter 1 --blr 1.5e-4 --weight_decay 0.05 --epochs 800 --warmup_epochs 5 
```

### Code reading

Q1: How do I use my dataset?

A1: Please change the --data_path and modify the data load code if needed in [main_pretrain.py](https://github.com/waterdisappear/SARATR-X/blob/main/pre-training/main_pretrain.py) and [datasets.py](https://github.com/waterdisappear/SARATR-X/blob/main/pre-training/util/datasets.py).

```python
    # Dataset parameters
    parser.add_argument('--data_path', default='D:\\2023_SARatrX_1\Pre-Train Data\\186K_notest\\', type=str,
                        help='dataset path')
                        
    from util.datasets import load_data
    # dataset_train = datasets.ImageFolder(os.path.join(args.data_path), transform=transform_train)
    dataset_train = load_data(os.path.join(args.data_path), transform=transform_train)
    print(len(dataset_train))
```

Q2: How do we make improvements?

A2: You can add more high-quality data and try more data augment methods. Besides, we suggest improvements to the HiViT's attention mechanism in [models_hivit.py](https://github.com/waterdisappear/SARATR-X/blob/main/pre-training/models/models_hivit.py) and our proposed SAR target features in [models_hivit_mae.py](https://github.com/waterdisappear/SARATR-X/blob/main/pre-training/models/models_hivit_mae.py).

```python
    # simple augmentation
    transform_train = transforms.Compose([
            transforms.RandomResizedCrop(args.input_size, scale=(0.2, 1.0), interpolation=3),  # 3 is bicubic
            transforms.RandomHorizontalFlip(),
            transforms.ColorJitter(contrast=0.5),
            transforms.ToTensor(),
            # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])
    ])
    
    # SAR feature 
    self.sarfeature1 = GF(nbins=self.nbins,pool=self.cell_sz,kensize=9,
                              img_size=self.img_size,patch_size=self.patch_size)
    self.sarfeature2 = GF(nbins=self.nbins,pool=self.cell_sz,kensize=13,
                              img_size=self.img_size,patch_size=self.patch_size)
    self.sarfeature3 = GF(nbins=self.nbins,pool=self.cell_sz,kensize=17,
                              img_size=self.img_size,patch_size=self.patch_size)
    target = torch.cat([self.patchify(self.sarfeature1(imgs)), self.patchify(self.sarfeature2(imgs)), self.patchify(self.sarfeature3(imgs))], dim=-1)
```
Q3: How to load ImageNet pre-training weights?

A3: You can see in [main_pretrain.py](https://github.com/waterdisappear/SARATR-X/blob/main/pre-training/main_pretrain.py).

```python
    # define the model
    model = models.__dict__[args.model](norm_pix_loss=args.norm_pix_loss)
    checkpoint = torch.load('./mae_hivit_base_1600ep.pth',
                            map_location='cpu')
    # load pre-trained model
    msg = model.load_state_dict(checkpoint, strict=False)
    print(msg)
```

## Classification

## Detection

## Statement

- If you have any questions or need additional data, code and weight files, please contact us at lwj2150508321@sina.com. 
- If you find our work is useful, please give us ğŸŒŸ in GitHub and cite our paper in the following BibTex format:

- å¦‚æœ‰ä»»ä½•é—®é¢˜æˆ–è€…éœ€è¦å…¶ä»–æ•°æ®ã€ä»£ç å’Œæƒé‡æ–‡ä»¶ï¼Œè¯·é€šè¿‡ lwj2150508321@sina.com è”ç³»æˆ‘ä»¬ã€‚
- å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰ä»·å€¼ï¼Œè¯·åœ¨ GitHub ä¸Šç»™æˆ‘ä»¬ ğŸŒŸ å¹¶æŒ‰ä»¥ä¸‹ BibTex æ ¼å¼å¼•ç”¨æˆ‘ä»¬çš„è®ºæ–‡ï¼š
```
@article{li2024saratr,
  title={SARATR-X: Towards Building A Foundation Model for SAR Target Recognition},
  author={Li, Weijie and Yang, Wei and Hou, Yuenan and Liu, Li and Liu, Yongxiang and Li, Xiang},
  journal={arXiv preprint},
  url={https://arxiv.org/abs/2405.09365},
  year={2024}
}

@article{li2024predicting,
  title = {Predicting gradient is better: Exploring self-supervised learning for SAR ATR with a joint-embedding predictive architecture},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {218},
  pages = {326-338},
  year = {2024},
  issn = {0924-2716},
  doi = {https://doi.org/10.1016/j.isprsjprs.2024.09.013},
  url = {https://www.sciencedirect.com/science/article/pii/S0924271624003514},
  author = {Li, Weijie and Yang, Wei and Liu, Tianpeng and Hou, Yuenan and Li, Yuxuan and Liu, Zhen and Liu, Yongxiang and Liu, Li},
}
```
